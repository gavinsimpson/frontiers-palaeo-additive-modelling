---
title: "Modelling palaeoecological time series using generalised additive models"
author: Gavin L. Simpson
#author:
#- name: "Gavin L Simpson"
#  affiliation: Insitute of Environmental CHange and Society, University of Regina, Regina, SK
date: November 3, 2016
bibliography: references.bib
output:
  pdf_document:
    fig_width: 10
    fig_height: 7
    fig_crop: false
    keep_tex: true
    latex_engine: xelatex
    template: my.latex
    md_extensions: +header_attributes+superscript+subscript
    includes:
      in_header: header.tex
#    pandoc_args: [
#      "--no-tex-ligatures"
#    ]
#    template: frontiers-template.tex
# documentclass: frontiersSCNS # not yet ready for this; need to pandoc-ify the template
---

```{r knitr-defaults, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment=NA, fig.align = "center", out.width = "0.8\\linewidth",
                      echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)
knitr::knit_hooks$set(crop.plot = knitr::hook_pdfcrop)
```

```{r load-packages, cache = FALSE}
library("mgcv")
library("ggplot2")
library("cowplot")
library("viridis")
library("grid")                         # for unit.pmax(), unit.list()
library("gstat")                        # for hscat & variogram
library("sp")                           # for coordinates()
```

```{r common-components, cache = FALSE}
## Set up a default palette
pal <- viridis(3)

## Default ggplot theme
theme_set(theme_bw())

## load the simultaneous confidence interval derivatives code
tmpf <- tempfile()
download.file("https://gist.githubusercontent.com/gavinsimpson/ca18c9c789ef5237dbc6/raw/295fc5cf7366c831ab166efaee42093a80622fa8/derivSimulCI.R",
              tmpf, method = "wget")
source(tmpf)

## Source other functions
source("functions/simulate.gamm.R")
```

## Abstract

350 Words

## Introduction

* intro about palaeo time series

A linear model for a trend in a series of observations $y_t$ at observation times $x_t$ with $t = 1, 2, \ldots, T$ is

\begin{equation} \label{eq:linear-model}
y_t = \beta_0 + \beta_1 x_t + \varepsilon_t,
\end{equation}

where $\beta_0$ is a constant term, the model *intercept* representing the expected value of $y$ where $x$ is 0. $\beta_1$ is the *slope* of the best fit line through the data; it measures the rate of change in $y$ for a unit increase in $x$. The unknowns, the $\beta_j$ are commonly estimated using least squares by minimising the sum of squared errors, $\varepsilon^2$. If we want to ask if the estimated trend $\beta_1$ is statistically significance, a process commonly called *inference*, we make further assumptions about the data (conditional upon the fitted model) or the model errors (or residuals); $\varepsilon_t \stackrel{iid}{\sim} \mathcal{N}(0, \sigma^2)$. This notation indicates that the residuals $\varepsilon_t$ are *independent* and *identically distributed* Gaussian random variables with mean equal to 0 and constant variance $\sigma^2$. In the time series setting, the assumption of independence of model residuals is often violated.

The linear model described above is quite restrictive in terms of the types of trend it can fit; essentially linear increasing or decreasing trends, or trivially, a null trend of no change. This model can be extended to allow for non-linear trends, most notably via making $y_t$ depend on polynomials of $x_t$, for example

\begin{align} \label{eq:polynomial-model}
y_t &= \beta_0 + \beta_1 x_t + \beta_2 x_t^2 + \cdots + \beta_p x_t^p + \varepsilon_t \\
    &= \beta_0 + \sum_{p = 1}^P \beta_p x_t^p  + \varepsilon_t\\
    &= \boldsymbol{\beta} \mathbf{X} + \boldsymbol{\varepsilon}
\end{align}

where polynomials of $x_t$ up to order $P$ are used. This model allows for more complex fitted trends but it remains a fully parametric model and suffers from several problems, especially the behaviour of the fitted polynomial at the start and end of the observed series.

## Generalised additive models

A generalised additive models (GAM) is a generalisation of the generalised linear model (GLM), of which the linear model described above is a special case. In a GAM, the parametric terms, the $\beta_p$ are replace by a smooth function $f()$ of $x_t$

\begin{equation} \label{eq:additive-model}
y_t = \beta_0 + f(x_t) + \varepsilon_t
\end{equation}

The immediate advantage of the GAM is that we are no longer restricted to the types of trends that can be fitted via global polynomial models such as \eqref{eq:polynomial-model}. Instead, the form of the fitted trend is estimated from the data itself.

For completeness, where the data are non-Gaussian, such as for where $y_t$ represents a time series of counts of species, we can write \eqref{eq:additive-model} as a GLM/GAM

\begin{align} \label{eq:gam}
g(\mu) &= \beta_0 + f(x_t) \\
\mu    &= g^{-1}(\beta_0 + f(x_t)),
\end{align}

where $\mu$ is the expected value of the random variable $Y$ of which we have observations $y_t$. $g$ is the link function, an invertable, monotonic function, such as the natural logarithm, and $g^{-1}$ its inverse. The link function maps values from the linear predictor scale on to the scale of the response. For example, count data are strictly non-negative and are commonly modelled as a Poisson GLM/GAM using the natural log link function, the inverse of which, the exponential function, maps values to the interval 0--$\infty$. In \eqref{eq:gam}, we further assume that the observations are drawn from a member of the exponential family of distributions, such as the Poisson for count data, the binomial for presence/absence or counts from a total. Additionally, many software implementations of the above models now allow for the fitting of distributions that are not within the exponential family but which can be fitted using an algorithm superficially similar to the one used to fit GAMs to members of the exponential family. Common examples of such extended families include the negative binomial and beta distributions.

### Basis functions

### Smoothness selection

### Smoothing time series data

## δ^15^N time series from Small Water

```{r small-water-example}
small <- readRDS("./data/small-water/small-water-isotope-data.rds")

## Generate a plot of the data - used as a base later
plt <- ggplot(small, aes(x = Year, y = d15N)) +
    geom_point(colour = pal[1]) +
    theme_bw() +
    ylab(expression(delta^{15}*N ~(Bulk~Organic~Matter)))

mod <- gamm(d15N ~ s(Year, k = 15), data = small,
            correlation = corCAR1(form = ~ Year), method = "REML")

## Predict from the fitted model
newYear <- with(small, data.frame(Year = seq(min(Year), max(Year), length.out = 100)))
newYear <- cbind(newYear, data.frame(predict(mod$gam, newYear, se.fit = TRUE)))

## Need to do this manually, not through `fd` - write a function
set.seed(1)
nsim <- 10000
take <- 50
sims <- simulate(mod, nsim = nsim, newdata = newYear, unconditional = TRUE)
randSims <- sims[, sample(nsim, take)]
colnames(randSims) <- paste0("sim", seq_len(take))
randSims <- setNames(stack(as.data.frame(randSims)), c("simulated", "run"))
randSims <- transform(randSims, Year = rep(newYear$Year, take),
                      simulated = simulated)

## Add a CI to the fitted spline
CI <- apply(sims, 1L, quantile, probs = c(0.025, 0.975))
newYear <- transform(newYear, lower = CI[1,], upper = CI[2,])

## Plot simulated trends
plt.sim <- ggplot(newYear, aes(x = Year, y = fit)) +
    geom_line(data = randSims, mapping = aes(y = simulated, x = Year, group = run),
              colour = pal[1], alpha = 0.1) +
    geom_line(colour = pal[1]) +
    theme_bw() +
    ylab(expression(delta^{15}*N ~(Bulk~Organic~Matter))) +
    xlab("Year")

## Derivatives
fd <- derivSimulCI(mod, samples = 10000, n = 100)
CI <- apply(fd[[1]]$simulations, 1, quantile, probs = c(0.025, 0.975))
sigD <- signifD(fd[["Year"]]$deriv, fd[["Year"]]$deriv, CI[2, ], CI[1, ],
                eval = 0)
newYear <- transform(newYear,
                     derivative = fd[["Year"]]$deriv[, 1], # computed first derivative
                     fdUpper = CI[2, ],                    # upper CI on first deriv
                     fdLower = CI[1, ],                    # lower CI on first deriv
                     increasing = sigD$incr,               # where is curve increasing?
                     decreasing = sigD$decr)               # ... or decreasing?

## Plot it
derivPlt <- ggplot(newYear, aes(x = Year, y = derivative)) +
    geom_ribbon(aes(ymax = fdUpper, ymin = fdLower), alpha = 0.3, fill = pal[1]) +
    geom_line(colour = pal[1]) +
    geom_line(aes(y = increasing), colour = pal[1], size = 1.5) +
    geom_line(aes(y = decreasing), colour = pal[1], size = 1.5) +
    ylab(expression(italic(hat(f) * "'") * (Year))) +
    xlab("Year")

## Plot of data, fitted spline, CI, and significant periods of change
## Need the signifD helper function to generate the vectors for increasing
## and decreasing periods
sigDYear <- signifD(newYear$fit, fd[["Year"]]$deriv[,1], CI[2, ], CI[1, ],
                    eval = 0)
newYear <- transform(newYear,
                     YearIncr = sigDYear$incr,
                     YearDecr = sigDYear$decr)

plt.fit2 <- ggplot(small, aes(x = Year, y = d15N)) +
    geom_ribbon(data = newYear,
                mapping = aes(ymax = upper, ymin = lower, y = fit, x = Year),
                fill = pal[1], alpha = 0.3) +
    geom_line(data = newYear, aes(y = fit, x = Year), colour = pal[1]) +
    geom_point(colour = pal[1]) +
    theme_bw() +
    ylab(expression(delta^{15}*N ~(Bulk~Organic~Matter))) +
    geom_line(data = newYear, mapping = aes(y = YearIncr), size = 1.5) +
    geom_line(data = newYear, mapping = aes(y = YearDecr), size = 1.5)

## plot each in a 2x2 display
smallWaterPlt <- plot_grid(plt, plt.sim, derivPlt, plt.fit2, labels = "AUTO",
                           align = "vh")

## summary object for use in document
smallSumm <- summary(mod$gam)

## estimate of phi and confidence interval
smallPhi <- intervals(mod$lme, which = "var-cov")$corStruct

## plot CAR(1) process
maxS <- with(small, diff(range(Year))) ## too large, truncate to 50
S <- seq(0, 50, length = 100)
phi <- intervals(mod$lme, which = "var-cov")$corStruct
car1 <- setNames(as.data.frame(t(outer(phi, S, FUN = `^`)[1, , ])),
                 c("Lower","Correlation","Upper"))
car1 <- transform(car1, S = S)

car1Plt <- ggplot(car1, aes(x = S, y = Correlation)) +
    geom_ribbon(aes(ymax = Upper, ymin = Lower),
                fill = "grey", alpha = 0.3) +
    geom_line() +
    ylab(expression(italic(h) * (list(Delta[t], phi)))) +
    xlab(expression(Delta[t]))
```

```{r small-water-display, fig = TRUE, fig.cap = "Stable isotopes"}
smallWaterPlt
```

Figure \ref{fig:small-water-display}A shows `r nrow(small)` nitrogen stable isotope measurements from the bulk organic matter of a core collected from Small Water, a small corrie lake located in the English Lake District, UK. The data are shown on a ^210^Pb time scale. Common questions that might be asked about this series are; what is the trend in δ^15^N?, when do we first see evidence for a change in δ^15^N?, and is the reversal in δ^15^N values in the uppermost section of the core a real change?

The trend in δ^15^N values is clearly non-linear and it would be difficult to suggest a suitable polynomial model that would allow for periods of relatively no change in δ^15^N as well as rapid change. Instead, a GAM is ideally suited to modelling such trends; the data suggest a smoothly varying change in δ^15^N between 1925 and 1975. It is reasonable to expect some autocorrelation in the model errors about the fitted trend. Therefore I fitted the following GAM to the δ^15^N time series.

\begin{equation} \label{eq:small-gam}
y_t = \beta_0 + f(x_t) + \varepsilon, \quad \varepsilon \sim \mathcal(0, \boldsymbol{\Lambda}\sigma^2)
\end{equation}

Note that now I have relaxed the i.i.d. assumption and have introduced $\boldsymbol{\Lambda}$, a corrleation matrix that is used to model any remaining autocorrelation in the residuals. The Small Water δ^15^N values are irregularly spaced in time and as a result a correlation structure that can handle the uneven spacing of the samples. A continuous time first-order autoregressive process (CAR(1)) is a reasonable choice; it is the continuous-time equivalent of the autoregressive process and simple stated states that the correlation between any two residuals is $\phi^d$, where $d$ is the amount of separation in time between the residuals. $d$ may be a real valued number in the CAR(1), which accomodates the irregular separation of samples. $\phi$ controls how quickly the correlation between any two residuals declines as a function of their separation in time. $\phi$ is an additional paramter that will be estimated during model fitting. The model in \ref{eq:small-gam} was fitted using the `gamm()` function in the **mgcv** package for R.

The fitted trend is shown in Figure \ref{fig:small-water-display}B, and well-captures the strong pattern in the data. The trend is statistically significant (estimated degrees of freedom = `r round(smallSumm[["s.table"]][1,2], 3)`; $F$ = `r round(smallSumm[["s.table"]][1,3], 3)`, approximate $p$ value = $\ll$ 0.0001). However further analysis of the fitted model is required to answer the other questions posed earlier about the timing of change and whether features in the trend can be distinguished from random noise.

### Simultaneous confidence intervals

The first step in answer these and other questions is to form a confidence about the fitted trend. Standard theory can be used to compute point-wise intervals. Such intervals have the appropriate 1 - $\alpha$ confidence for a single point only and when considering multiple points, as we will here, suffer from the problem of multiple comparison; when considered over all observations or points, the confidence interval is much smaller than the required 1 - $\alpha$.

Instead a 1 - $\alpha$ *simultaneous* confidence interval is required. One way to compute this interval is via posterior simulation from the fitted model. As described above, the GAM involves finding estimates for the coefficients of the spline basis functions given the value of the smoothing parameter. These spline coefficients form a multivariate normal distribution with mean vector and covariance matrix specified by the model estimates of the cofficients and their covariances. Having parameterised the multivariance normal distribution, random draws from the distribution can be taken. Each random draw represents a new trend that is consistent with the consistent with the fitted trend but it also includes the uncertainty in the estimated trend. Figure \ref{fig:small-water-display}B shows `r take` random trends drawn from the posterior distribution of the fitted model.

The simultaneous confidence interval itself is based on `r format(nsim, big.mark = ",")` samples from the model posterior. The fitted value of the trend for each of the posterior samples was evaluated at `r nrow(newYear)` equally spaced time points over the period of observation. At each point, the 0.025 and 0.975 probability quantiles of the distribution of posterior samples was obtained; these extrema contain 95% of the simulated trends and thus give the 95% simultaneous confidence interval on the fitted trend. This interval is shown in Figure \ref{fig:small-water-display}D.

### Identifying periods change

In the simple linear trend model \eqref{eq:linear-model} whether the estimated trend constitutes evidence for or against a null hypothesis of no change rests on how large the estimated rate of change in $y_t$ is ($\hat{\beta}_1$) relative to its uncertainty. This is summarised in the $t$ statistic. As the rate of change in $y_t$ is constant over the fitted trend --- there is only a singe slope for the fitted trend $\hat{\beta}_1$ --- if the $t$ statistic of the test that $\hat{\beta}_1 = 0$ is unusually extreme we can conclude that the trend is statistically significant and this applies to the entire time series of data. The estimate $\hat{\beta}_1$ is also the first derivative of the fitted trend.

In the GAM, the fitted trend is non-linear and hence the slope of the trend is potentially different at every point in the time series. As such we can reasonably ask where in the fitted trend is the response $y_t$ changing. Mirroring the simple linear model we can ask whether answer this question by determining whether or not the first derivative, the slope, of the fitted trend at any time point is consistent with a null hypothesis of no change; i.e. whether or not the first derivative is indistinguishable from a value of 0 given the uncertainty in the estimate of the derivative.

Derivatives of the fitted spline are not easily available analytically, but they can be estimated using the method of finite differences. Two values, separated by a very small time-shift $\Delta_t$, of the fitted trend are predicted from the model; the difference between the fitted values for the two time points is an approximation of the true derivative of the trend. As $\Delta_t \rightarrow 0$ the approximation becomes more and more accurate. The derivative of the fitted trend is evaluated at large number of time points over the series. An approximate 1 - $\alpha$ pointwise confidence interval can be calculated for the derivative estimates using standard theory and the covariance matrix of the spline coefficients.

Instead, a 1 - $\alpha$ simultaneous confidence interval can be computed using the posterior simulation method described above. This was done for the Small Water model for the `r format(nsim, big.mark = ",")` simulated trends by evaluating them at a large number of time points and computing the finite differences. The 0.025 and 0.975 probability quantiles of the distributions of derivative estimates at each evaluation point were computed and form the 95% simultaneous confidence for the first derivative of the fitted trend. The first derivative of the fitted trend and its 95% simultaneous confidence interval infor the δ^15^N time series are shown in Figure \ref{fig:small-water-display}C. Although the data and fitted trend suggest a slight increase in δ^15^N between 1850  and 1925, the estimated first derivative is consistent with a value of 0; hence there is little evidence against the null hypothesis of no change in δ^15^N over this period.

From ~1925 the derivative of of the trend becomes negative, however it is not until ~1940 that the confidence interval doesn't include 0. At this point we have evidence to reject the null hypothesis of no change. This time point is taken as the first evidence for change in δ^15^N in the Small Water core. The confidence interval on the first derivatve of the trend in δ^15^N is bounded away from 0 between ~1940 and 1975, covering the major decline in values clearly evident in the observations. This period of significant change is indicated by bold sections on the fitted trend in Figure \ref{fig:small-water-display}C and D. The confidence interval includes 0 between 1975 and around the year 2000, after which time the estimated first derivative becomes positive, suggesting a small but significant increase in the δ^15^N over the last 10 years of the time series, although caution must exercised when interpreting N stable isotope values in the partly decomposed organic matter commonly encountered in the most recent period of the sedimentary record.

Figure Figure \ref{fig:small-water-display}D summarises the main elements of the GAM analysis of the Small Water δ^15^N time series showing the i) fitted trend, ii) 95% simultaneous confidence interval, and iii) estimated periods of change.

### Residual autocorrelation

```{r car1-plot, fig = TRUE, fig.height = 7, fig.width = 10, fig.cap = "Estimated CAR(1) process from the GAM fitted to the Small Water $\\delta^{15}\\text{N}$ time series. $h(\\Delta_t, \\phi)$ is the correlation between residuals separated by $\\Delta_t$ years, where $\\hat{\\phi} = \\text{0.6}$. The shaded band is a 95\\% confidence interval on the estimated correlation $h$."}
car1Plt
```

The estimated value of $\phi$ from the fitted model is `r round(smallPhi[2], 2)` with 95% confidence interval `r round(smallPhi[1], 2)`--`r round(smallPhi[3], 2)`, indicating moderate to strong residual autocorrelation about the fitted trend. Failure to account for the dependencies in the δ^15^N time series could lead have lead to the estimation of a more wiggly trend than the one shown in Figure \ref{fig:small-water-display}B which would negatively impact the confidence place on the inferences we might draw from the fitted model. Importantly, failing to account for the strong dependency in the residuals would lead to smaller uncertainties in the estimated spline coefficients, which would propagate through to narrower confidence intervals on the fitted trend and on the first derivatives, and ultimately to the identification of significant periods of change in the δ^15^N time series.

## Braya-sø

```{r braya-so-example}
## Source other functions
source("functions/simulate.gam.R")

## Data
braya <- read.table("./data/braya-so/DAndrea.2011.Lake Braya So.txt", skip = 84)
names(braya) <- c("Depth", "DepthUpper", "DepthLower", "Year", "YearYoung", "YearOld", "UK37")
braya <- transform(braya, sampleInterval = YearYoung - YearOld)

## Generate a plot of the data
ylabel <- expression(italic(U)[37]^{italic(k)})
brayaData.plt <- ggplot(braya, aes(x = Year, y = UK37)) +
    geom_point(colour = pal[1]) +
    geom_line(colour = pal[1]) +
    theme_bw() +
    ylab(ylabel)

## fit the car(1) model
braya.car1 <- gamm(UK37 ~ s(Year, k = 20), data = braya, correlation = corCAR1(form = ~ Year),
                   method = "REML")
braya.gcv <- gam(UK37 ~ s(Year, k = 30), data = braya)

## car(10 parameter
brayaPhi <- intervals(braya.car1$lme)$corStruct

## ggplot with data and fitted spline, then resids vs time in second panel
ylabel <- expression(italic(U)[37]^{italic(k)})
N <- 300
newYear <- with(braya, data.frame(Year = seq(min(Year), max(Year), length.out = N)))
newYear <- cbind(newYear, data.frame(predict(braya.car1$gam, newYear, se.fit = TRUE)))
crit.t <- qt(0.975, df = df.residual(braya.car1$gam))
newYear <- transform(newYear,
                     upper = fit + (crit.t * se.fit),
                     lower = fit - (crit.t * se.fit))
## add GAM GCV results
fit.gcv <- predict(braya.gcv, newdata = newYear, se.fit = TRUE)
newYear <- rbind(newYear, newYear)      # extend newYear to take GCV results
newYear[seq(N+1, length.out = N, by = 1), ]$fit <- fit.gcv$fit
newYear[seq(N+1, length.out = N, by = 1), ]$upper <-
    fit.gcv$fit + (qt(0.975, df.residual(braya.gcv)) * fit.gcv$se.fit)
newYear[seq(N+1, length.out = N, by = 1), ]$lower <-
    fit.gcv$fit - (qt(0.975, df.residual(braya.gcv)) * fit.gcv$se.fit)
newYear <- transform(newYear, Method = rep(c("GAMM (CAR(1))", "GAM (GCV)"), each = N))

## plot CAR(1) and GCV fits
braya.p1 <- ggplot(braya, aes(y = UK37, x = Year)) +
    geom_point() +
    geom_ribbon(data = newYear,
                mapping = aes(y = fit, x = Year, ymax = upper, ymin = lower, fill = Method),
                alpha = 0.4) +
    geom_line(data = newYear, mapping = aes(y = fit, x = Year, colour = Method), size = 1) +
    ylab(ylabel) +
    theme_bw() +
    scale_color_manual(values = viridis(11)[c(2, 6)]) +
    scale_fill_manual(values = viridis(11)[c(2, 6)]) +
    theme(legend.position = "top")

## Generate GCV and REML traces
lambda <- 1e-8
gcv <- reml <- numeric(length = 100)

for (i in seq_along(gcv)) {
    mGCV <- gam(UK37 ~ s(Year, k = 30), data = braya, method = "GCV.Cp", sp = lambda)
    mREML <- gam(UK37 ~ s(Year, k = 30), data = braya, method = "REML", sp = lambda)
    gcv[i] <- mGCV$gcv.ubre
    reml[i] <- mREML$gcv.ubre
    lambda <- lambda * 1.5
}

## GCV and REML score traces
result <- data.frame(lambda = 1e-8 * 1.5^{0:99}, score = c(gcv, reml),
                     criterion = rep(c("GCV", "REML"), each = 100))

## Full fits to get estimated models
fullGCV  <- gam(UK37 ~ s(Year, k = 30), data = braya, method = "GCV.Cp")
fullREML <- gam(UK37 ~ s(Year, k = 30), data = braya, method = "REML")
obsSP <- data.frame(lambda = c(fullGCV$sp, fullREML$sp), criterion = c("GCV", "REML"))

## Plot
trace.plt <- ggplot(result, aes(x = lambda, y = score)) +
    geom_line() +
    facet_wrap(~ criterion, scales = "free_y") + theme_bw() + scale_x_log10() +
    xlab(expression(lambda)) +
    ylab("Score") +
    geom_vline(aes(xintercept = lambda), obsSP, colour = viridis(3)[2])

g.braya.p1 <- ggplotGrob(braya.p1) # convert to gtable
g.trace.plt <- ggplotGrob(trace.plt) # convert to gtable

if(getRversion() < "3.3.0"){
  # We need to convert the widths to unit lists for the subsequent
  # manipulations to be possible.
  # Once R 3.3.0 is released, this will not be necessary anymore.
  g.braya.p1$widths <- grid:::unit.list(g.braya.p1$widths)   
  g.trace.plt$widths <-  grid:::unit.list(g.trace.plt$widths)
}

braya.p1.widths <- g.braya.p1$widths[1:3] # extract the first three widths, 
                                          # corresponding to left margin, y lab, and y axis
trace.plt.widths <- g.trace.plt$widths[1:3] # same for trace.plt plot
max.widths <- unit.pmax(braya.p1.widths, trace.plt.widths) # calculate maximum widths
g.braya.p1$widths[1:3] <- max.widths # assign max widths to braya.p1 gtable
g.trace.plt$widths[1:3] <- max.widths # assign max widths to trace.plt gtable

# plot_grid() can work directly with gtables, so this works
brayaFail.plt <- plot_grid(g.braya.p1, g.trace.plt, labels = "AUTO", ncol = 1, rel_heights = c(0.55, 0.45))
```

The second example time series is a 6,000 year record of alkenone unsaturation, \uk, from Braya-sø, a meromictic lake in  West Greenland [@DAndrea2011-oa]. Alkenones are long-chained unsaturated organic compounds that are produced by a small number of planktonic organisms known as haptophytes. The \uk{} unsaturation index is

\begin{equation*}
\uk = \frac{[C_{37:2}] - [C_{37:4}]}{[C_{37:2}] + [C_{37:3}]+ [C_{37:4}]}
\end{equation*}

where $[C_{37:x}]$ is the concentration of the alkenone with 37 carbon atoms and $x$ double carbon bonds. The relative abundance of these alkenones is known to vary with changes in water temperature, and as a result \uk{} is used as a proxy for lake- and sea-surface temperatures. For further details on the Braya-sø \uk{} record and age model see @DAndrea2011-oa. Here I use the 3,000 year \uk{} record from the PAGES 2K database [@PAGES_2k_Consortium2013-fm]. The data are presented in Figures \ref{fig:braya-so-car1-gcv-fits}A and \ref{fig:braya-display}A.

```{r braya-so-car1-gcv-fits, cache = FALSE, fig = TRUE, fig.height = 7, fig.width = 10, fig.cap = "Fitted trends using CAR(1) and GCV models"}
brayaFail.plt
```

The \uk{} data present a more difficult data analysis challenge than the δ^15^N time series from Small Water because of the much more complex variations present. Fitting the same model as the Small Water example, \ref{eq:small-gam}, to the \uk{} data resulted in the unsatisfactory fit shown as the very smooth line in Figure \ref{fig:braya-so-car1-gcv-fits}A. Further problems were evident with this model fit --- the covariance matrix of the variance-covariance terms in the model was non-positive definite, a sure sign of problems with the fitted model. Refitting with a smaller basis dimension (*k* = 20) for the trend term resulted in a model with a positive-definite covariance matrix for the model variance-covraiance terms, but the estimate of the CAR(1) parameter $\phi$ = `r round(brayaPhi[2], 2)` was exceedingly uncertain (95% confidence interval `r round(brayaPhi[1], 2)` -- `r round(brayaPhi[3], 2)`!)

Fitting this model as a standard GAM with REML smoothness selection resulted in the same fitted trend as the GAM with CAR(1) errors (not shown), whilst using GCV smoothness selection resulted in a much more satisfactory fitted trend, yet normally, we would prefer REML smoothness selection. There are two potential problems with the GCV-selected trend: i) GCV is sensitive to the profile of the GCV score and has been shown to badly undersmooth data in situations where the profile is flat around the minimum GCV score, and ii) the model fitted assumes that the observations are independent, an assumption that is certainly violated in the \uk{} time series.

The GCV and REML scores for an increasing sequence of values of the smoothness penalty $\lambda$ were evaluated for for the standard GAM fit to the \uk{} time series. The resulting profiles are shown in Figure \ref{fig:braya-so-car1-gcv-fits}B, with the selected value of the penalty shown by the vertical line. The GCV score profile suggests that the potential for undersmoothing identified by Wood is unlikely to apply here. Addressing the violation of the independence assumption is more difficult; the standard errors of the model coefficients are most likely anti-conservative, which presents a significant problem if we wish to determine which of the wiggles in the fitted \uk{} trend can be identified as real.

To understand the reason why the GAM plus CAR(1) and the GAM with REML smoothness selection performed poorly with the \uk{} time series we need to delve a little deeper into what is happening when we are fitting these two models.

### Identifiability and induced correlations

Consider again the basic GAM for a smooth trend, \eqref{eq:additive-model}. In that equation the correlation matrix $\boldsymbol{\Lambda}$ was ommitted for the sake of simplicity. Here I reintroduce it and restate the distributional assumptions of this model

\begin{equation}
y_t = \beta_0 + f(x_t) + \varepsilon_t, \quad \varepsilon \sim \mathcal(0, \boldsymbol{\Lambda}\sigma^2)
\end{equation}

In the basic GAM, $\boldsymbol{\Lambda} \equiv \mathbf{I}$ is an identity matrix, a matrix with 1s on the diagonal and 0s elsewhere. This is where the independence assumption of the model comes from; a model residual is perfectly correlated with itself (the 1s on the diagonal), but uncorrelated with any other residual (the off-diagonal 0s). In the GAM plus CAR(1) model, I specified an alternative correlation function for $\boldsymbol{\Lambda}$ --- the CAR(1) with correlation parameter $\phi$. @Fahrmeir2008-si show that where the stochastic structure of $f$ and $\boldsymbol{\Lambda}$ approach one another, where we have a potentially wiggly trend or strong autocorrelation as $\phi \rightarrow 1$, the two processes can quickly become unidentifiable [see also @Fahrmeir2013-xu **give pages**]. By unidentifiable, I mean that it becomes increasingly difficult to distinguish between a wiggly trend or strong autocorrelation because these two processes are very similar to one another in appearance.

Recall that the estimate of the autocorrelation parameter in the CAR(1) for the \uk{} series was $\hat{\phi}$ = `r round(brayaPhi[2], 2)`. It seems unlikely therefore that the CAR(1) process and the trend are unidentifiable in this instance. The large degree of uncertainty in $\hat{\phi}$ suggests that the fitted model is too complex, containing too many similar terms. That the GAM plus CAR(1) model and the GAM with REML smoothness selection, which lacks the CAR(1) process, both resulted in the same fitted trend suggests that the problem lies elsewhere.

When fitting GAMs using REML smoothness selection, the splines are treated as random effects. More specifically, the entirely smooth components of the spline basis enter the model as fixed effects and the wiggly or non-smooth basis functions enter as random effects terms. Choosing the smoothness parameter $\lambda$ is then a matter of estimating variance of the random effect terms. If you are familiar with mixed effects models, you will know that random effects incude correlations between observations. In a mixed effects moedl with a simple random intercept term for a grouping variable, the induced correlation is

$$\rho = \sqrt{\sigma^2_{B} / \sigma^2_{T}}$$

where $\sigma^2_{B}$ is the between group variance (or the variance of the distribution of random intercepts), and $\sigma^2_{T}$ is the total variance in the data, and $\rho$ is the correlation between any two observations in the same group.

Treating the wiggly parts of a spline basis as random effects results in similar but more complex induced correlations [@Fahrmeir2008-si]. It seems likely that these induced correlations between observations are modelling components of the data which we might rightly wish to consider as trend. These induced correlations provide a complex structure to $\boldsymbol{\Lambda}$, and the identifiability issue discussed previously again applies, but from a slightly different source.

```{r induced-correlations, cache = FALSE}
knitr::read_chunk("./analysis/braya-so/braya-so-induced-correlations.R")
```
```{r induced-correlations-plt, fig = TRUE, fig.cap = "Induced correlations"}
```

Ideally, we would use a method account for autocorrelation in the data and yet not suffer from the same identifiability problems described here. In the next section I look at one such model, which goes some way to addressing this issue.

### Gaussian process smooths

```{r fit-braya-example-gp-smooths}
m.reml <- gam(UK37 ~ s(Year, k = 30), data = braya, method = "REML")
resi.reml <- data.frame(Residuals = resid(m.reml), Year = braya$Year, Dummy = 1)
coordinates(resi.reml) <- ~ Year + Dummy
lagScatter <- hscat(Residuals ~ 1, data = resi.reml, breaks = c(0, 25, 50, 100,200, 500, 1000))
resi.vgmCloud <- variogram(Residuals ~ 1, data = resi.reml, cloud = TRUE)
resi.vgm <- variogram(Residuals ~ 1, data = resi.reml, width = 20, cressie = TRUE)

variogPlt <- ggplot(resi.vgm, aes(x = dist, y = gamma)) +
    geom_smooth(se = FALSE, col = "grey", span = 0.75,
                method.args = list(family = "gaussian")) +
    geom_point() + 
    xlim(c(0, 1000)) + ylim(c(0, 1e-3)) +
    ylab(expression(Semivariance ~~ gamma)) +
    xlab("Temporal separation (years)")

## Fit gaussian process spline models
effRange <- 250                         # sets effective range in years for Matern correl
mod.gp <- gam(UK37 ~ s(Year, k = 30, bs = "gp", m = c(3, effRange)), data = braya,
              method = "REML")
## Add time interval of each sample as prior weights; effectively averaging more time per sample
mod2.gp <- gam(UK37 ~ s(Year, k = 30, bs = "gp", m = c(3, effRange)), data = braya,
               method = "REML", weights = sampleInterval)

## Predict from model
N <- 500
newBraya <- with(braya, data.frame(Year = seq(min(Year), max(Year), length.out = N)))
newBraya <- cbind(newBraya, data.frame(predict(mod.gp, newBraya, se.fit = TRUE)))

## Draw the fitted spline on the data
ylabel <- expression(italic(U)[37]^{italic(k)})
brayaFit.plt <- ggplot(braya, aes(x = Year, y = UK37)) +
    geom_line(data = newYear, aes(y = fit, x = Year), colour = pal[1]) +
    geom_point(colour = pal[1]) +
    theme_bw() +
    ylab(ylabel)

## posterior simulation
set.seed(1)
nsim <- 10000
take <- 50
sims <- simulate(mod.gp, nsim = nsim, newdata = newBraya, unconditional = TRUE)
randSims <- sims[, sample(nsim, take)]
colnames(randSims) <- paste0("sim", seq_len(take))
randSims <- setNames(stack(as.data.frame(randSims)), c("simulated", "run"))
randSims <- transform(randSims, Year = rep(newBraya$Year, take),
                      simulated = simulated)

## Add a CI to the fitted spline
CI <- apply(sims, 1L, quantile, probs = c(0.025, 0.975))
newBraya <- transform(newBraya, lower = CI[1,], upper = CI[2,])

brayaSim.plt <- ggplot(newBraya, aes(x = Year, y = fit)) +
    geom_line(data = randSims, mapping = aes(y = simulated, x = Year, group = run),
              colour = pal[1], alpha = 0.1) +
    geom_line(colour = pal[1]) +
    theme_bw() +
    ylab(ylabel) +
    xlab("Year")

## Derivatives
fd <- derivSimulCI(mod.gp, samples = 10000, n = N)
CI <- apply(fd[[1]]$simulations, 1, quantile, probs = c(0.025, 0.975))
sigD <- signifD(fd[["Year"]]$deriv, fd[["Year"]]$deriv, CI[2, ], CI[1, ],
                eval = 0)
newBraya <- transform(newBraya,
                     derivative = fd[["Year"]]$deriv[, 1], # computed first derivative
                     fdUpper = CI[2, ],                    # upper CI on first deriv
                     fdLower = CI[1, ],                    # lower CI on first deriv
                     increasing = sigD$incr,               # where is curve increasing?
                     decreasing = sigD$decr)               # ... or decreasing?

## Plot it
braya.derivPlt <- ggplot(newBraya, aes(x = Year, y = derivative)) +
    geom_ribbon(aes(ymax = fdUpper, ymin = fdLower), alpha = 0.3, fill = pal[1]) +
    geom_line(colour = pal[1]) +
    geom_line(aes(y = increasing), colour = pal[1], size = 1.5) +
    geom_line(aes(y = decreasing), colour = pal[1], size = 1.5) +
    ylab(expression(italic(hat(f) * "'") * (Year))) +
    xlab("Year") +
    theme_bw()

## Plot of data, fitted spline, CI, and significant periods of change
## Need the signifD helper function to generate the vectors for increasing
## and decreasing periods
sigDYear <- signifD(newBraya$fit, fd[["Year"]]$deriv[,1], CI[2, ], CI[1, ],
                    eval = 0)
newBraya <- transform(newBraya,
                     YearIncr = sigDYear$incr,
                     YearDecr = sigDYear$decr)

brayaFit2.plt <- ggplot(braya, aes(x = Year, y = UK37)) +
    geom_ribbon(data = newBraya,
                mapping = aes(ymax = upper, ymin = lower, y = fit, x = Year),
                fill = pal[1], alpha = 0.3) +
    geom_line(data = newBraya, aes(y = fit, x = Year), colour = pal[1]) +
    geom_point(colour = pal[1]) +
    theme_bw() +
    ylab(ylabel) +
    geom_line(data = newBraya, mapping = aes(y = YearIncr), size = 1.5, colour = pal[1]) +
    geom_line(data = newBraya, mapping = aes(y = YearDecr), size = 1.5, colour = pal[1])

brayaPlt <- plot_grid(brayaData.plt, brayaSim.plt, braya.derivPlt, brayaFit2.plt, labels = "AUTO",
                      align = "vh")
```



```{r variogram-braya, fig = TRUE, fig.cap = "Variogram", cache = FALSE}
variogPlt
```

```{r braya-display, fig = TRUE, fig.cap = "Braya-sø", cache = FALSE}
brayaPlt
```

### Sediment compaction & time averaging

## Further considerations

### Multivariate data

## Discussion

## References
